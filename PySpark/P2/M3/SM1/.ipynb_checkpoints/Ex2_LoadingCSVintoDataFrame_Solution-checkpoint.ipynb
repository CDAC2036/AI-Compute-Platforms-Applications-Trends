{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CSV into DataFrame\n",
    "\n",
    "- In the previous exercise, you have seen a method of creating DataFrame but generally, loading data from CSV file is the most common method of creating DataFrames. In this exercise, you'll create a PySpark DataFrame from a `people.csv` file that is already provided to you as a `file_path` and confirm the created object is a PySpark DataFrame.\n",
    "\n",
    "- Remember, you already have `SparkSession` `spark` and `file_path` variable (which is the path to the `people.csv` file) available in your workspace.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- Create a DataFrame from `file_path` variable which is the path to the `people.csv` file.\n",
    "- Confirm the output as PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of people_df is <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "file_path = \"file:///home/talentum/test-jupyter/P2/M3/SM1/Dataset/people.csv\"\n",
    "\n",
    "# Create an DataFrame from file_path\n",
    "'''\n",
    "csv(path, schema=None, sep=None, encoding=None, ..., header=None, inferSchema=None, dateFormat=None, timestampFormat=None,... )\n",
    "\n",
    "Loads a CSV file and returns the result as a DataFrame.\n",
    "\n",
    "This function will go through the input once to determine the input schema if inferSchema is enabled. To avoid going through the entire data once, disable inferSchema option or specify the schema explicitly using schema.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "path – string, or list of strings, for input path(s), or RDD of Strings storing CSV rows.\n",
    "\n",
    "schema – an optional pyspark.sql.types.StructType for the input schema or a DDL-formatted string (For example col0 INT, col1 DOUBLE).\n",
    "\n",
    "sep – sets a single character as a separator for each field and value. If None is set, it uses the default value, ,.\n",
    "\n",
    "encoding – decodes the CSV files by the given encoding type. If None is set, it uses the default value, UTF-8.\n",
    "\n",
    "header – uses the first line as names of columns. If None is set, it uses the default value, false.\n",
    "\n",
    "inferSchema – infers the input schema automatically from data. It requires one extra pass over the data. If None is set, it uses the default value, false.\n",
    "\n",
    "dateFormat – sets the string that indicates a date format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to date type. If None is set, it uses the default value, yyyy-MM-dd.\n",
    "\n",
    "timestampFormat – sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd'T'HH:mm:ss.SSSXXX.\n",
    "\n",
    "'''\n",
    "\n",
    "people_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check the type of people_df\n",
    "print(\"The type of people_df is\", type(people_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- person_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- date of birth: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(people_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.session.SparkSession'>\n"
     ]
    }
   ],
   "source": [
    "print(type(spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------------+------+-------------+\n",
      "|_c0|person_id|             name|   sex|date of birth|\n",
      "+---+---------+-----------------+------+-------------+\n",
      "|  0|      100|   Penelope Lewis|female|   1990-08-31|\n",
      "|  1|      101|    David Anthony|  male|   1971-10-14|\n",
      "|  2|      102|        Ida Shipp|female|   1962-05-24|\n",
      "|  3|      103|     Joanna Moore|female|   2017-03-10|\n",
      "|  4|      104|   Lisandra Ortiz|female|   2020-08-05|\n",
      "|  5|      105|    David Simmons|  male|   1999-12-30|\n",
      "|  6|      106|    Edward Hudson|  male|   1983-05-09|\n",
      "|  7|      107|     Albert Jones|  male|   1990-09-13|\n",
      "|  8|      108| Leonard Cavender|  male|   1958-08-08|\n",
      "|  9|      109|   Everett Vadala|  male|   2005-05-24|\n",
      "| 10|      110| Freddie Claridge|  male|   2002-05-07|\n",
      "| 11|      111|Annabelle Rosseau|female|   1989-07-13|\n",
      "| 12|      112|    Eulah Emanuel|female|   1976-01-19|\n",
      "| 13|      113|       Shaun Love|  male|   1970-05-26|\n",
      "| 14|      114|Alejandro Brennan|  male|   1980-12-22|\n",
      "| 15|      115|Robert Mcreynolds|  male|   1973-12-27|\n",
      "| 16|      116|   Carla Spickard|female|   1985-06-13|\n",
      "| 17|      117|Florence Eberhart|female|   2024-06-01|\n",
      "| 18|      118|     Tina Gaskins|female|   1966-12-05|\n",
      "| 19|      119| Florence Mulhern|female|   1959-05-31|\n",
      "+---+---------+-----------------+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "show(n=20, truncate=True, vertical=False)\n",
    "\n",
    "Prints the first n rows to the console.\n",
    "\n",
    "Parameters:\n",
    "n – Number of rows to show.\n",
    "\n",
    "truncate – If set to True, truncate strings longer than 20 chars by default. If set to a number greater than one, truncates long strings to length truncate and align cells right.\n",
    "\n",
    "vertical – If set to True, print output rows vertically (one line per column value).\n",
    "'''\n",
    "\n",
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
